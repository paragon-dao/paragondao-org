// Verification Knowledge Base â€” Article Data
// 9 articles forming a learning path about health AI verification
// Content blocks: paragraph, heading, callout, formula, comparison, diagram, list, tryit, divider

const learnArticles = [
  {
    slug: 'why-verify',
    title: 'Why Verify Health AI?',
    subtitle: "You wouldn't take a pill that wasn't tested. Why trust an AI model that wasn't?",
    order: 1,
    category: 'Foundations',
    readingTime: '3 min read',
    excerpt: 'Unverified AI models can misdiagnose, discriminate, and fail silently. Verification is how you know a model actually works on people it has never seen before.',
    tags: ['Trust', 'Safety', 'Fundamentals'],
    content: [
      { type: 'paragraph', text: "Before a new drug reaches your pharmacy shelf, it goes through years of testing. Clinical trials, independent review boards, FDA approval. Nobody would swallow a pill that skipped all that. So why do we accept AI models that haven't been tested the same way?" },
      { type: 'callout', variant: 'info', title: 'What Is an AI Model?', text: "An AI model is a piece of software that has been trained on data to make predictions \u2014 in this case, predictions about health. Think of it as a program that learned from examples instead of being hand-coded by a programmer." },
      { type: 'heading', text: 'The Trust Gap' },
      { type: 'paragraph', text: "Machine learning papers routinely claim 95% or 99% accuracy. Sounds great. But here's the problem: most of those numbers come from testing the model on data it's already seen, or data that's suspiciously similar to what it trained on. It's like a student who memorized the answer key acing the test. The grade looks perfect, but it doesn't mean they understand the material." },
      { type: 'paragraph', text: "When that model meets a real patient it has never seen before \u2014 someone from a different age group, a different background, a different hospital \u2014 performance can collapse. And in health AI, a silent failure can mean a missed diagnosis." },
      { type: 'callout', variant: 'warning', title: 'What Can Go Wrong', text: "A skin cancer detection model trained mostly on light-skinned patients may miss melanomas on darker skin (documented in peer-reviewed dermatology research). An EEG model that memorized specific patients' brain patterns may fail completely on new patients. These aren't hypothetical \u2014 they're known failure modes." },
      { type: 'heading', text: 'What Verification Actually Proves' },
      { type: 'paragraph', text: "Verification answers one question: does this model work on people it has never seen before?" },
      { type: 'paragraph', text: "Not 'does it work on the same patients it trained on.' Not 'does it work on a cherry-picked test set.' Does it generalize? That means: has it learned actual patterns in human biology, rather than memorizing the specific people in its training data?" },
      { type: 'paragraph', text: "To prove this, you need strict rules: completely separate groups of people for training and testing, standardized scoring methods that can't be gamed, and independent validators who re-run the tests. That's what our verification process does." },
      { type: 'callout', variant: 'info', title: 'Who Is ParagonDAO?', text: "ParagonDAO is building an open verification network for health AI models. We believe every model that touches patient data should prove it works \u2014 publicly, independently, and permanently. Our encoder, the General Learning Encoder (GLE), reads brain signals and predicts cognitive states like focus, drowsiness, and sleep stages." },
      { type: 'callout', variant: 'key', title: 'Key Takeaway', text: "Verification is how you know a model works on people it's never seen before. Without it, accuracy numbers are meaningless." },
    ]
  },
  {
    slug: 'how-we-test',
    title: 'How We Test Models',
    subtitle: 'Independent validators test every model the same way, and anyone can check their work.',
    order: 2,
    category: 'Process',
    readingTime: '5 min read',
    excerpt: 'Our 7-step verification process ensures every health model is tested independently, transparently, and reproducibly \u2014 like peer review for AI.',
    tags: ['Pipeline', 'Validators', 'Transparency'],
    content: [
      { type: 'paragraph', text: "Think of model verification like grading homework \u2014 except multiple independent teachers grade every paper, they all use the same answer key, and the grades are published permanently so anyone can check." },
      { type: 'heading', text: 'The 7-Step Process' },
      { type: 'paragraph', text: "Every model that enters the ParagonDAO verification system goes through the same seven steps. No shortcuts, no special treatment." },
      { type: 'list', variant: 'ordered', items: [
        { title: 'Register Model', text: 'The builder uploads a description of what the model does, what data it was trained on, and what claims it makes.' },
        { title: 'Upload Artifact', text: "The model's internal parameters \u2014 the numbers that define how it makes predictions \u2014 are uploaded and stored encrypted. Nobody can tamper with them after submission." },
        { title: 'Select Benchmarks', text: 'The builder chooses which standard tests apply. For brain-signal models, this means a benchmark based on one of the largest AI competitions in the world (more on this in Article 8).' },
        { title: 'Independent Verification', text: 'Multiple independent validators each download the model and run the evaluation separately. They don\'t communicate with each other.' },
        { title: 'Results & Certification', text: 'If validators agree (within tolerance), the model receives a certification tier based on its performance.' },
        { title: 'Permanent Publication', text: 'Results are published to a permanent, tamper-proof public record that can never be altered.' },
        { title: 'List on Exchange', text: 'Verified models become available on the Exchange \u2014 a marketplace where other builders can discover, compare, and integrate verified models into their own health applications.' },
      ]},
      { type: 'heading', text: 'Who Are the Validators?' },
      { type: 'paragraph', text: "Validators are independent computers, each run by a different operator, that put up a financial deposit as a guarantee of honest behavior. If a validator produces results that disagree with the others, their deposit is at risk. This economic incentive keeps everyone honest without needing a central authority." },
      { type: 'callout', variant: 'info', title: 'Current Status', text: "The validator network is currently in development. Initial verification is performed by ParagonDAO's internal pipeline following the same methodology described here. As the network grows, independent validators will take over \u2014 making the system fully decentralized." },
      { type: 'heading', text: "What 'Independently Verifiable' Means" },
      { type: 'paragraph', text: "Every step produces a paper trail \u2014 logs, scores, intermediate results \u2014 that anyone can download and re-run. You don't have to trust us. You don't have to trust the validators. You can verify everything yourself." },
      { type: 'tryit', text: 'See the pipeline in action', href: '/proof-pipeline' },
      { type: 'callout', variant: 'key', title: 'Key Takeaway', text: "Independent validators test every model the same way, and anyone can check their work." },
    ]
  },
  {
    slug: 'the-score',
    title: 'The Score That Matters',
    subtitle: 'Our model has 29% less error than guessing the average. Here\u2019s what that actually means.',
    order: 3,
    category: 'Metrics',
    readingTime: '4 min read',
    excerpt: 'The Normalized Error metric tells you exactly how much better a model is than simply guessing the average. Here\'s what our score means and why it matters.',
    tags: ['Metrics', 'NeurIPS', 'Performance'],
    content: [
      { type: 'paragraph', text: "Imagine a weather forecaster. The simplest possible forecast is to always predict tomorrow's temperature will be the average temperature for this time of year. Some days that's close; some days it's way off. But it's the simplest possible guess \u2014 the minimum viable prediction." },
      { type: 'paragraph', text: "In AI, we call this simplest-possible prediction the baseline. Now imagine a smarter forecaster who actually looks at weather patterns. How much less error does their forecast have compared to always guessing the average? That's exactly what the Normalized Error metric measures for AI models." },
      { type: 'paragraph', text: "Our model is the General Learning Encoder (GLE) \u2014 it reads brain signals and predicts cognitive states like focus, drowsiness, and sleep stages. Here's how we measure its performance." },
      { type: 'heading', text: 'The Number Line' },
      { type: 'paragraph', text: "The score runs on a simple scale:" },
      { type: 'diagram', content: '  0.0        0.709          0.978   1.0      >1.0\n   |            |              |      |        |\n  Perfect   ParagonDAO    NeurIPS   Just    Worse\n  prediction   GLE        winner  guessing  than\n                                  average  guessing\n\n  \u2190\u2014\u2014\u2014 better \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 worse \u2014\u2014\u2014\u2192' },
      { type: 'paragraph', text: "Lower is better. A score of 1.0 means the model is no better than always predicting the average. Below 1.0, the model is learning real patterns. Above 1.0, the model is actually worse than just guessing." },
      { type: 'heading', text: 'For Those Who Want the Math' },
      { type: 'formula', expression: 'NRMSE = RMSE(model) / RMSE(mean_baseline)', explanation: "NRMSE stands for Normalized Root Mean Square Error. RMSE measures how far off predictions are, on average. Divide the model's error by the error you'd get from always predicting the average. If the model's error is smaller, the ratio falls below 1.0 \u2014 meaning the model is doing better than guessing." },
      { type: 'heading', text: 'What Our Score Means' },
      { type: 'comparison', items: [
        { label: 'ParagonDAO GLE', value: '0.709', detail: '29.1% less error than baseline', variant: 'good' },
        { label: 'NeurIPS Challenge Winner', value: '0.978', detail: '2.2% less error than baseline', variant: 'neutral' },
      ]},
      { type: 'paragraph', text: "The gap between our model and the baseline was over 13x larger than the challenge winner's gap. Both beat the baseline, but our improvement margin was dramatically larger. This matters because in health applications, every percentage point of accuracy can affect real patients." },
      { type: 'paragraph', text: "The NeurIPS EEG Foundation Model Challenge attracted over a thousand teams worldwide. This isn't a made-up benchmark \u2014 it's one of the most competitive open evaluations in biosignal AI." },
      { type: 'callout', variant: 'key', title: 'Key Takeaway', text: "A score of 0.709 means our model has 29% less error than just guessing the average. The competition winner had only about 2% less error." },
    ]
  },
  {
    slug: 'data-splits',
    title: "Why People Can't Be in Two Groups",
    subtitle: "If a student sees the test answers during studying, their grade doesn't mean anything.",
    order: 4,
    category: 'Methodology',
    readingTime: '4 min read',
    excerpt: "Data leakage is the most common way AI evaluations go wrong. Here's how subject-level splits prevent cheating and prove a model actually learned something real.",
    tags: ['Data', 'Leakage', 'Splits'],
    content: [
      { type: 'paragraph', text: "Imagine a teacher who accidentally includes some of the test questions in the homework. Students who memorized the homework answers will ace the test \u2014 but they didn't actually learn the material. That's data leakage, and it's the single most common way AI evaluations go wrong." },
      { type: 'heading', text: 'Two Ways to Split Data' },
      { type: 'paragraph', text: "When testing a health AI model, you need to divide your data into groups: one for training (what the model learns from), one for testing (the final exam). There's also a third group called validation \u2014 a practice test used to tune the model during development, separate from both training and the final exam. But how you divide people into these groups matters enormously." },
      { type: 'comparison', items: [
        { label: 'Sample-Level Splits', value: 'WRONG', detail: "Randomly shuffle all data samples. Person A's Monday data might end up in training while their Tuesday data ends up in testing. The model recognizes Person A's patterns and 'cheats' on the test.", variant: 'bad' },
        { label: 'Subject-Level Splits', value: 'CORRECT', detail: "All of Person A's data goes in one group only. If Person A is in the test group, zero of their data appears in training. The model must work on people it has literally never seen.", variant: 'good' },
      ]},
      { type: 'heading', text: 'How We Check for Zero Overlap' },
      { type: 'paragraph', text: "Our verification script runs three overlap checks, comparing every possible combination: training vs. validation, training vs. test, and validation vs. test. If any person appears in more than one group, verification fails immediately. No exceptions." },
      { type: 'paragraph', text: "The current dataset uses 20 subjects from the Healthy Brain Network: 14 for training, 3 for validation, and 3 for testing. Zero overlap between any groups." },
      { type: 'callout', variant: 'info', title: 'NeurIPS Requirement', text: "The NeurIPS EEG Foundation Model Challenge explicitly requires: 'No subject\\'s data is spread across multiple releases.' Our protocol enforces this programmatically." },
      { type: 'heading', text: 'Why This Matters for You' },
      { type: 'paragraph', text: "If a health model was tested using sample-level splits, its reported accuracy is inflated. It might say 95% but drop to 60% on a real new patient. Subject-level splits give you the real number \u2014 the accuracy you can actually expect." },
      { type: 'callout', variant: 'key', title: 'Key Takeaway', text: "Every person's data stays in exactly one group. If the model works on people it never trained on, it actually learned something real." },
    ]
  },
  {
    slug: 'privacy-attacks',
    title: 'Can Someone Steal Patient Data from a Model?',
    subtitle: 'We tried three standard ways to extract patient information from our model. All three failed.',
    order: 5,
    category: 'Privacy',
    readingTime: '5 min read',
    excerpt: 'A trained model might accidentally memorize the patients who trained it. Privacy attacks test whether anyone can extract that information. Our model passed all three.',
    tags: ['Privacy', 'Security', 'Attacks'],
    content: [
      { type: 'paragraph', text: "Here's a scary question: if you give someone a trained AI model, can they figure out whose data was used to train it? Can they reconstruct what that data looked like? In health AI, where the training data comes from real patients, this isn't theoretical \u2014 it's a genuine privacy risk." },
      { type: 'paragraph', text: "Why does this matter practically? If an attacker can determine that your brain data was used to train a mental health model, they could infer that you sought mental health treatment. In health AI, data leakage is not just a technical problem \u2014 it's a privacy violation with real-world consequences." },
      { type: 'paragraph', text: "The responsible approach is to assume the worst and test for it. We run three standard privacy attacks against every verified model. Think of them as three different types of break-in attempts." },
      { type: 'heading', text: 'Attack 1: Was This Person in the Training Data?' },
      { type: 'callout', variant: 'info', title: 'Membership Inference Attack', text: "Question: 'Was John's data used to train this model?' The attacker tries to determine whether a specific person's data was in the training set, based on how the model responds to their data. (Based on the foundational work of Shokri et al., 2017.)" },
      { type: 'paragraph', text: "Imagine a private party. An outsider watches who comes and goes, trying to figure out who was actually on the guest list versus who just happened to walk by. That's membership inference \u2014 testing whether the attacker can tell who was 'invited' (used in training) versus who wasn't." },
      { type: 'paragraph', text: "If a model memorizes its training data, it will behave slightly differently on training patients vs. new patients. This attack exploits that difference. A perfectly private model would show no difference at all \u2014 giving the attacker a 50/50 guess (random chance)." },
      { type: 'paragraph', text: "Our model scored 0.512 accuracy for the attacker \u2014 barely above the 0.500 of a coin flip. A vulnerable model might score 0.8 or higher, meaning the attacker could reliably identify training patients. Ours can't." },
      { type: 'heading', text: 'Attack 2: Can You Reconstruct the Original Data?' },
      { type: 'callout', variant: 'info', title: 'Model Inversion Attack', text: "Question: 'What did John's data look like?' The attacker tries to reconstruct the original input data by working backwards from the model's outputs, using mathematical reverse-engineering techniques. (Following the model inversion framework of Fredrikson et al., 2015, adapted for biosignal data.)" },
      { type: 'paragraph', text: "This is like trying to reconstruct a recipe by tasting the final dish. If the model retains too much detail about its ingredients, a skilled attacker might be able to work backwards to the original data." },
      { type: 'paragraph', text: "Our model scored 0.034 correlation between reconstructed and original inputs (zero means no recovery at all). The original data cannot be reverse-engineered from the model." },
      { type: 'heading', text: 'Attack 3: Whose Brain Is This?' },
      { type: 'callout', variant: 'info', title: 'Attribute Inference Attack', text: "Question: 'Whose brain does this data belong to?' The attacker tries to identify which specific person produced a given data sample, using the model's internal analysis of the data." },
      { type: 'paragraph', text: "Our GLE encoder includes a privacy-protection mechanism that specifically trains the model to forget who the data came from. It learns brain patterns, not brain identities. (This defense uses a technique called domain-adversarial training, introduced by Ganin & Lempitsky, 2015.)" },
      { type: 'paragraph', text: "With 20 subjects in the dataset, random guessing gives 5% accuracy. Our model gave the attacker only 8.9% accuracy \u2014 a 3.9 percentage-point advantage over pure chance. The model has effectively erased individual identity from what it learns." },
      { type: 'callout', variant: 'key', title: 'Key Takeaway', text: "We tried three standard ways to extract patient information from our model. All three failed. The model learned patterns, not people." },
    ]
  },
  {
    slug: 'data-format',
    title: 'What the Data Actually Looks Like',
    subtitle: 'Four channels, two seconds, 800 numbers. That\u2019s what the model reads from your brain.',
    order: 6,
    category: 'Data',
    readingTime: '4 min read',
    excerpt: "EEG data comes from four sensor positions on the scalp, reading electrical signals 100 times per second. Here's what that data looks like and how GLE encoding transforms it.",
    tags: ['EEG', 'Data', 'Encoding'],
    content: [
      { type: 'paragraph', text: "So far, we've talked about how models are tested and protected. But what exactly is the model looking at? Understanding the raw data helps you understand why verification matters \u2014 and why it's hard." },
      { type: 'paragraph', text: "Your brain is constantly producing tiny electrical signals. Every thought, every sensation, every moment of focus or rest creates distinct patterns of electrical activity. An EEG (electroencephalogram) picks up these signals through sensors on your scalp." },
      { type: 'heading', text: 'The Four Channels' },
      { type: 'paragraph', text: "Our verification benchmark uses EEG data from four sensor positions, corresponding to where consumer-grade headbands like the Muse place their sensors:" },
      { type: 'diagram', content: '            AF7     AF8\n             \\       /\n              \\     /\n         TP9   Forehead   TP10\n           \\     |     /\n            \\    |    /\n             \\   |   /\n              \\  |  /\n               Ears\n\n   TP9  = Left ear (temporal)\n   AF7  = Left forehead (frontal)\n   AF8  = Right forehead (frontal)\n   TP10 = Right ear (temporal)' },
      { type: 'paragraph', text: "The benchmark data is sampled at 100 Hz (100 readings per second). Our standard analysis window is 2 seconds, which gives us 200 readings per channel. Four channels times 200 readings = 800 numbers. That's one data sample." },
      { type: 'callout', variant: 'info', title: 'About the Data Source', text: "The training data comes from the Healthy Brain Network (HBN), a large-scale clinical research project with over 3,000 participants. The original recordings use research-grade EEG systems with many more channels. We extract four channels matching the TP9/AF7/AF8/TP10 positions, downsampled to 100 Hz, so that verified models can work with both clinical and consumer-grade hardware." },
      { type: 'heading', text: 'Brain Radio Stations' },
      { type: 'paragraph', text: "Within those electrical signals, different frequencies carry different information \u2014 like different radio stations broadcasting simultaneously. Hz means 'cycles per second':" },
      { type: 'list', variant: 'unordered', items: [
        { title: 'Delta (0.5\u20134 Hz)', text: 'Deep sleep waves. Slow, rolling rhythms.' },
        { title: 'Theta (4\u20138 Hz)', text: 'Drowsiness and light meditation. The edge of consciousness.' },
        { title: 'Alpha (8\u201313 Hz)', text: 'Relaxed awareness. Close your eyes and alpha waves increase.' },
        { title: 'Beta (13\u201330 Hz)', text: 'Active thinking and focus. The faster your thoughts, the higher the frequency.' },
      ]},
      { type: 'heading', text: 'What GLE Encoding Does' },
      { type: 'paragraph', text: "The General Learning Encoder (GLE) transforms those 800 raw numbers into a compact fingerprint \u2014 128 summary values that capture the essential patterns while discarding noise and individual identity." },
      { type: 'paragraph', text: "The encoding process has four stages:" },
      { type: 'list', variant: 'ordered', items: [
        { title: 'Separate the frequencies', text: 'Like tuning into each radio station individually \u2014 breaks the signal into its component frequencies' },
        { title: 'Track changes over time', text: 'Identifies how the signal evolves across the 2-second window' },
        { title: 'Combine frequency and timing', text: 'Merges both types of information into one compact summary' },
        { title: 'Remove identity', text: 'A privacy-protection layer strips out individual-specific patterns, keeping only universal brain states' },
      ]},
      { type: 'callout', variant: 'key', title: 'Key Takeaway', text: "Four channels, two seconds, 800 numbers. That's what the model reads from your brain." },
    ]
  },
  {
    slug: 'certification-tiers',
    title: 'Bronze to Platinum: How Models Get Certified',
    subtitle: "Think of it like restaurant health grades \u2014 but for AI.",
    order: 7,
    category: 'Certification',
    readingTime: '3 min read',
    excerpt: 'Verified models receive a tier from Bronze to Platinum based on performance and privacy protection. Higher tiers mean the model is both more accurate and more private.',
    tags: ['Certification', 'Tiers', 'Trust'],
    content: [
      { type: 'paragraph', text: "When you walk into a restaurant, you might check its health inspection grade posted on the wall. A = great, B = acceptable, C = maybe eat somewhere else. Our certification tiers work the same way for AI models." },
      { type: 'heading', text: 'Four Tiers' },
      { type: 'comparison', items: [
        { label: 'Bronze', value: '> 70%', detail: 'The model works. It beats the baseline and produces useful predictions. Privacy testing is optional.', variant: 'neutral' },
        { label: 'Silver', value: '> 80%', detail: 'Reliable performance. The model generalizes well across different subjects. Privacy testing is optional but reported.', variant: 'neutral' },
        { label: 'Gold', value: '> 90%', detail: 'High performance with privacy protection. The model must pass all three privacy attacks with passing grades.', variant: 'good' },
        { label: 'Platinum', value: '> 95%', detail: 'Best-in-class. Highest accuracy AND strongest privacy. All three privacy attacks must score "STRONG" \u2014 meaning near-zero information leakage.', variant: 'good' },
      ]},
      { type: 'callout', variant: 'info', title: 'A Note on Metrics', text: "The percentages shown above are simplified thresholds. Different model types use different scoring methods \u2014 for brain-signal models, the normalized error score from Article 3 determines the tier. The principle is the same: higher tiers demand better performance." },
      { type: 'heading', text: 'Why Privacy Is Required at Higher Tiers' },
      { type: 'paragraph', text: "A model that's 95% accurate but leaks patient data is dangerous. At the Bronze and Silver level, we're testing whether the model works at all. But Gold and Platinum models are meant for real deployment with real patients \u2014 so they need to prove privacy protection too." },
      { type: 'heading', text: 'After Certification' },
      { type: 'paragraph', text: "Once certified, the model's results are published to a permanent, tamper-proof public record. The model is listed on the Exchange \u2014 a marketplace where other builders can discover and integrate it into their applications. Higher tiers get better visibility and trust signals." },
      { type: 'tryit', text: 'Browse verified models on the Exchange', href: '/exchange' },
      { type: 'callout', variant: 'key', title: 'Key Takeaway', text: "Platinum means the model is highly accurate AND your data can't be extracted from it. Both matter." },
    ]
  },
  {
    slug: 'neurips-competition',
    title: 'Tested Against the World\u2019s Best',
    subtitle: "We didn't just enter a competition \u2014 we built a permanent verification system based on its rules.",
    order: 8,
    category: 'Reference',
    readingTime: '5 min read',
    excerpt: 'The NeurIPS EEG Foundation Model Challenge attracted over a thousand teams worldwide. We adapted its rigorous evaluation protocol into an ongoing verification system.',
    tags: ['NeurIPS', 'Competition', 'Standards'],
    content: [
      { type: 'paragraph', text: "What if you could test your model against over a thousand teams from around the world \u2014 and then make those testing standards permanent? That's what we did with the NeurIPS EEG Foundation Model Challenge." },
      { type: 'paragraph', text: "NeurIPS (Neural Information Processing Systems) is one of the top AI research conferences in the world. Every year, it hosts competitions where teams compete to solve specific challenges. One of these challenges focused on brain signals \u2014 and it became the foundation for our verification system." },
      { type: 'heading', text: 'The HBN-EEG Dataset' },
      { type: 'paragraph', text: "The Healthy Brain Network (HBN) is a massive research project that has collected EEG recordings from over 3,000 children and teenagers. This data is public and standardized \u2014 making it an ideal benchmark for testing how well AI models understand brain activity." },
      { type: 'paragraph', text: "The NeurIPS challenge used a curated subset of this data with strict rules about how models could access it: separate releases for training, validation, and testing, with no subject's data appearing in multiple releases." },
      { type: 'heading', text: 'The Two Challenge Tracks' },
      { type: 'list', variant: 'ordered', items: [
        { title: 'Cross-task transfer', text: "Can a model trained on one type of brain task (like watching videos) predict brain activity during a different task (like resting with eyes closed)? This tests whether the model learned general brain patterns, not task-specific shortcuts." },
        { title: 'Subject-invariant prediction', text: "Can a model work on completely new people? This is the harder problem, because every brain is slightly different. A model that only works on the people it trained on is useless in the real world." },
      ]},
      { type: 'callout', variant: 'info', title: 'Scale', text: "Over a thousand teams from around the world competed, making it one of the largest open benchmarks for biosignal AI ever conducted." },
      { type: 'heading', text: 'From Competition to Permanent Verification' },
      { type: 'paragraph', text: "Most competitions are one-time events: teams submit, winners are announced, everyone goes home. We took a different approach. We adapted the NeurIPS evaluation protocol into an ongoing, permanent verification system." },
      { type: 'paragraph', text: "This means any builder, at any time, can submit a model and have it evaluated against the same rigorous standards that the competition used. The rules don't change based on who you are or when you submit. The benchmark is the benchmark." },
      { type: 'paragraph', text: "The key elements we carried forward:" },
      { type: 'list', variant: 'unordered', items: [
        { title: 'Subject-level splits', text: 'No data leakage between training and testing, as we covered in Article 4' },
        { title: 'Normalized error metric', text: 'Fair comparison across different tasks and models, as we covered in Article 3' },
        { title: 'Standardized data preparation', text: 'Everyone starts with the same cleaned-up version of the data \u2014 no custom tricks to inflate scores' },
        { title: 'Reproducible evaluation', text: 'Same script, same data, same metric for everyone' },
      ]},
      { type: 'callout', variant: 'key', title: 'Key Takeaway', text: "We didn't just enter a competition \u2014 we built a permanent verification system based on its rules." },
    ]
  },
  {
    slug: 'verify-yourself',
    title: 'How to Verify It Yourself',
    subtitle: "Don't trust us \u2014 test it. Every verification result is publicly accessible.",
    order: 9,
    category: 'Hands-On',
    readingTime: '4 min read',
    excerpt: "Don't take our word for it. Every verification result is publicly accessible. Here's how to check our work \u2014 whether you write code or not.",
    tags: ['API', 'Tutorial', 'DIY'],
    content: [
      { type: 'paragraph', text: "Everything we've described in the previous articles \u2014 the scores, the data splits, the privacy tests \u2014 is backed by publicly accessible verification data. Let's walk through how to check our work." },
      { type: 'callout', variant: 'info', title: 'Two Ways to Verify', text: "If you're a developer, the API endpoints below let you query verification data programmatically. If you're not a developer, skip straight to the Playground section \u2014 it lets you do everything through a visual interface, no code required." },
      { type: 'heading', text: 'For Developers: API Endpoints' },
      { type: 'paragraph', text: "An API endpoint is a web address you can call from code to get specific data back. Think of it like a vending machine: put in a request, get back a result. Here are the key endpoints:" },
      { type: 'heading', text: 'Check Verification Status' },
      { type: 'formula', expression: 'GET /api/verify/{modelId}/status', explanation: 'Returns the model\'s current verification status, certification tier, and all metric scores. Replace {modelId} with any model ID from the Exchange.' },
      { type: 'heading', text: 'View Subject Split Verification' },
      { type: 'formula', expression: 'GET /api/verify/{modelId}/splits', explanation: 'Returns the exact subject IDs in each group (train, validation, test) and confirms zero overlap. You can manually check that no subject appears in more than one group.' },
      { type: 'heading', text: 'View Privacy Attack Results' },
      { type: 'formula', expression: 'GET /api/verify/{modelId}/privacy', explanation: 'Returns scores for all three privacy attacks: membership inference accuracy, model inversion correlation, and attribute inference advantage. Lower is better for all three.' },
      { type: 'heading', text: 'Run a Live Prediction' },
      { type: 'formula', expression: 'POST /api/predict\nBody: { "eeg_data": [[...], [...], [...], [...]] }', explanation: 'Send raw EEG data (4 channels x 200 samples) and get back the model\'s prediction. Compare the result against known labels to verify accuracy yourself.' },
      { type: 'heading', text: 'Try the Playground' },
      { type: 'paragraph', text: "Don't want to write code? Use the interactive playground to run predictions with sample data, visualize brain states, and see verification results in real time." },
      { type: 'tryit', text: 'Open the Verification Playground', href: '/verify/eeg-consciousness?tab=playground' },
      { type: 'heading', text: 'Submit Your Own Model' },
      { type: 'paragraph', text: "Built a health model? Submit it for verification. You can run in simulation mode first (no real upload required) to see how the process works, then submit for real when you're ready." },
      { type: 'tryit', text: 'Submit a model for verification', href: '/forge/submit' },
      { type: 'callout', variant: 'key', title: 'Key Takeaway', text: "Every verification result is publicly accessible. Don't trust us \u2014 test it." },
    ]
  },
]

export default learnArticles

// Helper: get articles in learning path order
export const getLearningPath = () => [...learnArticles].sort((a, b) => a.order - b.order)

// Helper: get unique categories
export const getCategories = () => [...new Set(learnArticles.map(a => a.category))]

// Helper: get article by slug
export const getArticleBySlug = (slug) => learnArticles.find(a => a.slug === slug)
